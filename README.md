# Technical Specification Extraction using Local LLMs

> Assignment submission for LLM / AI Engineer role  
> Fully local, GPU-accelerated, no paid APIs

---

# üìÑ LLM-Based Specification Extraction from Technical Manuals

## üìå Project Overview

This project implements an **end-to-end local LLM pipeline** to extract **structured technical specifications** (e.g., torque values) from a **single large service manual PDF**.

The system is designed to:

* Work **fully offline** (no paid APIs)
* Run on **GPU servers (H100)**
* Produce output in an **exact predefined JSON schema**, as required by the assignment

### üéØ Target Output Format (Assignment Requirement)

```json
[
  {
    "component": "Brake Caliper Bolt",
    "spec_type": "Torque",
    "value": "35",
    "unit": "Nm"
  }
]
```

Only the **most relevant** result is returned per query.

---

## üß† High-Level Architecture

```
PDF ‚Üí Text Extraction ‚Üí Chunking
        ‚Üì
Embedding Model (BGE)
        ‚Üì
FAISS Similarity Search
        ‚Üì
Top-Relevant Chunk
        ‚Üì
Instruction-Tuned LLM
        ‚Üì
Strict JSON Output
```

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ extract_pipeline_final.py   # Main pipeline
‚îú‚îÄ‚îÄ pdfs/
‚îÇ   ‚îî‚îÄ‚îÄ sample.pdf              # Input service manual
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ Hermes-2-Pro/            # Local LLM (downloaded)
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îî‚îÄ‚îÄ results.json             # Final extracted output
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Environment & Dependencies

### Python

* Python **3.10**
* CUDA **12.8**
* PyTorch **2.9+cu128**

### Key Libraries

```bash
pip install torch transformers faiss-cpu pymupdf sentencepiece
pip install huggingface_hub
```

> `sentencepiece` is required for LLaMA/Mistral-based tokenizers.

---

## üöÄ How to Run

```bash
python3 extract_pipeline_final.py \
  --pdf pdfs/sample.pdf \
  --query "Torque for brake caliper bolts" \
  --out output/results.json
```

---

## üîç Embedding Model Selection

### ‚úÖ Final Choice

**`BAAI/bge-large-en-v1.5`**

**Why:**

* Excellent for **technical & engineering text**
* Better semantic retrieval than MPNet
* Strong performance on long manuals

### ‚ùå Previous Model

`sentence-transformers/all-mpnet-base-v2`

* Good general embeddings
* Less accurate for mechanical/engineering specs

---

## ü§ñ LLM Models Tested ‚Äî Full Evaluation Log

This section documents **all models tried**, issues faced, and final decisions.

---

### 1Ô∏è‚É£ `microsoft/phi-2`

**Status:** ‚ùå Failed
**Issues:**

* Generated Python code instead of JSON
* Weak instruction following
* Hallucinated values

**Verdict:** Too small for structured extraction

---

### 2Ô∏è‚É£ `tiiuae/falcon-7b-instruct`

**Status:** ‚ùå Failed
**Issues:**

* Verbose outputs
* Ignored strict JSON requirement
* Returned templates instead of real values

**Verdict:** Poor JSON compliance

---

### 3Ô∏è‚É£ `mistralai/Mistral-7B-Instruct`

**Status:** ‚ùå Failed
**Issues:**

* Gated / authentication issues
* Inconsistent extraction
* Weak component disambiguation

**Verdict:** Not reliable for assignment format

---

### 4Ô∏è‚É£ `meta-llama/Llama-3-8B-Instruct`

**Status:** ‚ùå Blocked
**Issues:**

* Gated model
* Requires license + manual approval
* Access not granted during assignment timeline

**Verdict:** Ideal model, but unavailable

---

### 5Ô∏è‚É£ `meta-llama/Llama-3.1-8B-Instruct`

**Status:** ‚ùå Pending approval
**Issues:**

* Requires Meta approval (manual review)
* 401 Unauthorized until approved

**Verdict:** Best possible model, but blocked by access delay

---

### 6Ô∏è‚É£ ‚úÖ **`NousResearch/Hermes-2-Pro-Mistral-7B` (FINAL)**

**Status:** ‚úÖ Success
**Why this worked:**

* Open access (no approval needed)
* Strong instruction tuning
* Excellent JSON compliance
* Correctly extracts multiple specs from same chunk
* Runs efficiently on H100

**Minor Issue (Solved):**

* Required `sentencepiece` for tokenizer

**Verdict:**
‚úÖ **Best open-source model for this task under constraints**

---

## üß™ Known Challenges & Fixes

| Issue                         | Root Cause                | Fix                      |
| ----------------------------- | ------------------------- | ------------------------ |
| NumPy crashes                 | Version mismatch          | Pinned NumPy 1.26.4      |
| SciPy / sklearn import errors | Transformers auto-import  | Uninstalled unused deps  |
| accelerate circular import    | Broken accelerate install | Removed accelerate       |
| Missing tokenizer             | No sentencepiece          | Installed sentencepiece  |
| Wrong torque selected         | Multiple specs in chunk   | Prompt + filtering logic |

---

## üß† Design Decisions

* **Top-1 retrieval** (not top-K) ‚Üí matches assignment example
* **Local models only** ‚Üí no paid APIs
* **Strict JSON enforcement** ‚Üí post-processing + validation
* **Modular pipeline** ‚Üí easy model replacement

---

## üìà Possible Improvements (Future Work)

* Cross-encoder reranking for chunks
* Table-aware PDF parsing
* Multi-query batch extraction
* Automatic unit normalization
* Support for multiple documents

---

## ‚úÖ Final Status

* ‚úî Fully local pipeline
* ‚úî GPU-accelerated
* ‚úî Deterministic output format
* ‚úî Assignment-compliant JSON
* ‚úî Reproducible & documented

---

## üë§ Author

**Anshul Chauhan**
LLM / AI Engineer Candidate


